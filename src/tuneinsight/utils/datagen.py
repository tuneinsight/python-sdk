"""Module to generate mock datasets in the Tune Insight instance."""

import json
from enum import Enum
import re
from typing import Union, List

import numpy as np

import pandas as pd

from tuneinsight.utils.code import get_code
from tuneinsight.utils.tracking import ProgressTracker, new_task_id
from tuneinsight.api.sdk.types import Response
from tuneinsight.api.sdk import Client

from tuneinsight.api.sdk.api.api_datagen import post_mock_dataset
from tuneinsight.api.sdk.models.post_mock_dataset_method import PostMockDatasetMethod

from tuneinsight.client.validation import validate_response
from tuneinsight.client.datasource import DataSource
from tuneinsight.client import Diapason

from .mbi.domain import available_converters, DatasetContext


class MockGenerator:
    """Generic class for a Mock generator. Use method-specific subclasses instead."""

    def __init__(self, method: PostMockDatasetMethod):
        self.method = method
        self.datasource = None

    def get_config(self) -> dict:
        """Get the optional configuration for this generator, to be sent in request body as JSON."""
        return {}

    def generate(
        self,
        client: Union[Diapason, Client],
        num_rows: int,
        table_name: str = None,
        seed: str = None,
        clear_if_exists: bool = True,
        track_progress: bool = False,
    ) -> DataSource:
        """
        Generates a mock dataset.

        Args:
            client (Client): Diapason client instance to connect to the server.
            num_rows (int): number of records to generate.
            table_name (str, optional): name of the database table to generate.
            seed (str, optional): seed of the pseudo-random number generator.
            clear_if_exists (bool, optional): whether to delete a datasource with the same name (default True).
            track_progress (bool, optional): whether to track the progress of the generation task

        Raises:
            httpx.TimeoutException: If the request takes longer than Client.timeout.
            InvalidResponseError: If the request to the server fails.

        Returns:
            DataSource: the datasource created, containing the mock data.

        """
        assert num_rows > -1, "num_rows must be nonnegative."
        # User-friendly interface: allow a Diapason object to be used here.
        if isinstance(client, Diapason):
            client: Client = client.client

        if table_name:
            # Replace spaces by underscores and remove all non-word characters.
            table_name = table_name.replace(" ", "_")
            table_name = "".join(re.findall("[\\w]+", table_name))
        else:
            table_name = f"mock_{self.method}"
        config: str = json.dumps(self.get_config())
        # If required, track progress asynchronously with a progress bar.
        task_id = new_task_id() if track_progress else None
        if track_progress:
            tracker = ProgressTracker(task_id)
            tracker.start_background(client)
        # Perform the API call.
        response: Response = post_mock_dataset.sync_detailed(
            client=client,
            json_body=config,
            method=self.method,
            name=table_name,
            numrows=num_rows,
            seed=seed,
            clear_if_exists=clear_if_exists,
            tracking_id=task_id,
        )
        validate_response(response=response)
        # The response contains the description of the datasource created by the call.
        self.datasource = DataSource(model=response.parsed, client=client)
        # Set the query on the datasource (since we know what table we want).
        self.datasource.set_local_query(f"select * from {table_name}")
        return self.datasource

    @property
    def df(self) -> pd.DataFrame:
        """Download the last dataframe generated by this generator."""
        if self.datasource is None:
            raise RuntimeError("No Dataframe found. Run the generator first.")
        return self.datasource.get_dataframe(
            query=f"SELECT * FROM {self.datasource.model.name}"
        )


class AlertsGenerator(MockGenerator):
    """Mock Alerts generation.

    This produces mock alerts from a network monitoring system. This has the attributes:
     - src_ip: source IP address (str).
     - dst_ip: destination IP address (str).
     - protocol: one of TCP, UDP or ICMP (str).
     - fingerprint: a SHA hash (str).
     - type: the attack type (str). Also 1-hot encoded.
     - severity: one of low, medium, high or critical (str). Also 1-hot encoded.

    """

    def __init__(self):
        MockGenerator.__init__(self, PostMockDatasetMethod.ALERTS)


class PatientsGenerator(MockGenerator):
    """Mock Patients generation.

    This produces mock simple patients records that can be used for survival analysis.
    The data has the following attributes:
     - patient_no: a pseudorandom identifier (str).
     - age: patient age (int).
     - sex: one of "female" or "male" (str).
     - height: patient height in cm (float).
     - weight: patient weight in kg (float).
     - observation: a disease/symptom, or empty string (str).
     - treatments: treatment received by the patient, or empty string (str).
     - diagnosis: timestamp at which an observation was recorded (timestamp).
     - death: timestamp at which the patient died or NaT if not dead (timestamp).

    """

    def __init__(self):
        MockGenerator.__init__(self, PostMockDatasetMethod.PATIENTS)


class NeurologyObservationsGenerator(MockGenerator):
    """Mock Neurology Observations Generation

    This produces mock neurological observations for patients. It has the attributes:
     - patient_id: a pseudorandom identifier (str).
     - disease_type: a neurological disease (str, 5 possible values).
     - diagnosis_dt: time of diagnosis (timestamp).
     - birthdate: birth date of the patient (timestamp).
     - pain_score: float between 0 and 11, or empty string (str).
     - mri_anomaly_detected: one of "NaN", "yes" or "no" (str).
     - surgery_required: one of "yes", "no", "" (str).

    """

    def __init__(self):
        MockGenerator.__init__(self, PostMockDatasetMethod.NEUROLOGY_OBSERVATIONS)


class PricesGenerator(MockGenerator):
    """Mock Prices Generation

    This produces a dataset of mock prices for commodities at a given time.
     - commodity (str): what is being sold.
     - delivery_start, delivery_end (date): period of delivery for this commodity.
     - currency (str): currency in which the price is expressed.
     - currency_multiplier (float): conversion rate to USD for the currency.
     - unit (str): unit in which the quantity is expressed.
     - unit_multiplier (float): conversion from the unit to a standard unit.
     - contributor (str): who is selling.
     - price (float): price per unit at which the commodity is sold.

    """

    contributors: List[str]

    def __init__(self, contributors: List[str] = None):
        MockGenerator.__init__(self, PostMockDatasetMethod.PRICES)
        self.contributors = contributors

    def set_contributors(self, contributors: List[str]):
        """
        Sets the custom list of contributors to use when generating the dataset.

        Args:
            contributors (List[str]): the list of contributors
        """
        self.contributors = contributors

    def get_config(self):
        if self.contributors is not None:
            return {"contributors": self.contributors}
        return {}


class SKUGenerator(MockGenerator):
    """Mock SKU Generation

    This produces mock stock keeping units (grocery products) with the following attributes:
     - sku_number: pseudorandom numerical value that is generated from the name of the product
     - product_name: name of the product.
     - product_type: type of product (spaghetti,chocolate etc...)
     - category: higher level category for the product (meat,drinks,dairy etc...)
     - manufacturer: name of the manufacturer.
     - supplier: name of the supplier.
     - price: price of individual product.
     - quantity: quantity produced for the current timestamp.
     - timestamp: the current timestamp.

    """

    typo_probability: float

    def __init__(self, typo_probability: float = 0.0):
        """
        Args:
            typo_probability (float, optional): the probability for typos in the product names. Defaults to None.
        """
        MockGenerator.__init__(self, PostMockDatasetMethod.SKUS)
        self.typo_probability = typo_probability

    def get_config(self):
        return {"typo_probability": self.typo_probability}


class PersonsGenerator(MockGenerator):
    """Mock Persons Generation

    This produces a mock dataset representing individuals with the following attributes:
     - name: full name of the person.
     - age: age of the person.
     - country: country in which the person lives.

    """

    def __init__(self):
        MockGenerator.__init__(self, PostMockDatasetMethod.PERSONS)


class CyberLogsGenerator(MockGenerator):
    """Cyber logs generation based on MISP RestSearch API with controlled collision probability.

    This produces mock logs from a network monitoring system. This has the attributes: "ip_src", "ip_dst", "url", "port", "hostname", "mac_address"
     - ip_src: source IP address (str).
     - ip_dst: destination IP address (str).
     - url: source url (str).
     - port:  source port.
     - hostname: source hostname.
     - mac_address: source mac address.

    """

    pool_size: int
    ipv6_prob: float

    def __init__(self, pool_size: int = 10000, ipv6_prob: float = 0.1):
        MockGenerator.__init__(self, PostMockDatasetMethod.CYBER_LOGS)
        self.pool_size = pool_size
        self.ipv6_prob = ipv6_prob

    def get_config(self) -> dict:
        return {
            "pool_size": self.pool_size,
            "ipv6_prob": self.ipv6_prob,
        }


class CustomFunctionGenerator(MockGenerator):
    """Custom Function Generation

    This generator enables users to specify an arbitrary Python function to generate
    mock records. The signature of the function must be () -> pd.DataFrame. The
    DataFrame it outputs contains one or more records related to one "user". This
    function is called until enough records are produced (if too many records are
    produced, the last output is downsampled such that the mock dataset has exactly
    the required size).

    The custom function should not use `import` statements, but can use the following
    libraries without importing them: datetime, dateutil, math, random, itertools,
    np (numpy), pd (pandas).

    The code of the custom function should be randomized and not seeded, as the
    generator takes care of seeding.

    Note: you can and should use this class as a decorator over the custom function.

    """

    def __init__(self, f):
        MockGenerator.__init__(self, PostMockDatasetMethod.CUSTOM_FUNCTION)
        self.f = f

    def get_config(self):
        return {"function": get_code(self.f)}

    def __call__(self):
        """Call the inner function of this generator (for decoration purposes)."""
        return self.f()


class GenericGenerator(MockGenerator):
    """
    Mock generator for data of a configurable format.

    This generator can be configured to produce mock tabular data of any format, and
    to add arbitrary distributions and correlations for attributes.

    This generator requires a data format describing the attributes. This can be either
    from a JSON file, or by manually adding columns through method columns.

    Additionally, _measurements_ (the simulated results of queries) can be provided
    to describe the distribution of the mock data, and _constraints_ can be applied to
    modify these measurements to apply high-level requirements.

    This object has three high-level objects that can be used to configure the generator:

    1. `attributes`: add and configure individual attributes (see `AttributeParser`),
    2. `measurements`: add and configure measurements, i.e., specifications of the attribute distribution (see `MeasurementParser`),
    3. `constraints`: add and configure high-level constraints on the distribution (see `ConstraintParser`).

    """

    attributes: "AttributeParser"
    measurements: "MeasurementParser"
    constraints: "ConstraintParser"

    def __init__(self, data_format=None):
        MockGenerator.__init__(self, PostMockDatasetMethod.GENERIC)
        self.measurement_list = []
        self.constraint_list = []
        self.data_format = data_format if data_format is not None else []
        # These objects provide an object-oriented interface to the configuration (the lists).
        # The attributes entry ingests the data format fed as argument.
        self.attrs = self.attributes = AttributeParser(self.data_format)
        self.msrs = self.measurements = MeasurementParser(
            self.measurement_list, self.attrs
        )
        self.ctrs = self.constraints = ConstraintParser(
            self.attrs, self.constraint_list
        )

    @classmethod
    def from_file(cls, data_format_file):
        """Load a Generic generator from a data format described in a JSON file."""
        with open(data_format_file, "r", encoding="utf-8") as ff:
            data_format = json.load(ff)
        return GenericGenerator(data_format)

    def save(self, json_filename):
        """Save the data format of this generator to a JSON file."""
        with open(json_filename, "w", encoding="utf-8") as ff:
            json.dump(self.data_format, ff)

    def get_config(self):
        return {
            "data-format": self.data_format,
            "measurements": self.measurement_list,
            "constraints": self.constraint_list,
        }


class AttributeParser:
    """
    Attribute parser for the generic mock generator.

    `AttributeParser` objects offer a high-level interface to create and validate the
    part of the configuration of the generic generator that defines attributes.
    """

    class TYPE(str, Enum):
        """Attribute types that are recognized by the generic generator."""

        CONTINUOUS = "continuous"
        INTEGER = "integer"
        CATEGORICAL = "categorical"
        DATE = "date"
        IDENTIFIER = "identifier"
        NAME = "name"

    # Maps the name of the attribute to a pair of (required, optional) parameters.
    FORMAT = {
        TYPE.CONTINUOUS: (
            {"min_value", "max_value"},
            {"bins", "allow_missing"},
        ),
        TYPE.INTEGER: (
            {"min_value", "max_value"},
            {"bins", "allow_missing"},
        ),
        TYPE.CATEGORICAL: ({"possible_values"}, set()),
        TYPE.DATE: ({"start_date", "end_date"}, {"bins", "strformat", "allow_missing"}),
        TYPE.IDENTIFIER: ({"pattern"}, set()),
        TYPE.NAME: (set(), {"pattern", "female_proportion", "locale"}),
    }

    def __init__(self, data_format):
        """Internal method -- use `GenericGenerator.attributes`."""
        # Check that the data format is valid.
        self.validate(data_format)
        # To embed domains and perform measurements, we instantiate local
        # converters for the data. This also keeps track of variable names.
        self.converters = {}
        # The same list is used for the data format, but it is "repopulated" instead
        # of keeping the same entries. This is used to set the internal state of this
        # object as if all entries had been added one-by-one.
        self.data_format = data_format
        _entries = list(data_format)  # Copy the list,
        self.data_format.clear()  # and empty the original's content.
        for attribute_config in _entries:
            self._add_attribute(**attribute_config)

    def validate(self, list_of_attributes: List[dict]):
        """Asserts that a data format is valid. Raises error if not."""
        for attr in list_of_attributes:
            assert "name" in attr, "Missing attribute name."
            assert "type" in attr, "Missing attribute type."
            c = AttributeParser.FORMAT.get(attr["type"])
            assert c is not None, f"Unknown type {c}."
            required, optional = c
            for req in required:
                assert req in attr, f"Missing {req} attribute for type {attr['type']}."
            allowed = required.union(optional.union({"name", "type"}))
            for v in attr:
                assert (
                    v in allowed
                ), f"Attribute {v} not allowed for type {attr['type']}."

    def _add_attribute(
        self, name: str, type: TYPE, **attributes  # pylint: disable=redefined-builtin
    ):
        assert name not in self.converters, f"Variable {name} already exists."
        # Remove optional arguments from the payload.
        # This is because providing a None (null) value will overwrite defaults.
        for optional_arg in AttributeParser.FORMAT[type][1]:
            if optional_arg in attributes and attributes[optional_arg] is None:
                del attributes[optional_arg]
        attr_values = {"name": name, "type": type, **attributes}
        self.validate([attr_values])
        converter = available_converters[type](**attributes)
        self.data_format.append(attr_values)
        self.converters[name] = converter

    # The following methods are intended as a public interface to this class.

    def add_continuous(
        self,
        name: str,
        min_value: float,
        max_value: float,
        bins: Union[int, list] = None,
        missing: bool = False,
    ):
        """
        Adds a continuous-valued attribute to the data description.

        Args:
            name: the name of this attribute (column) in the database.
            min_value: the minimum value that this attribute can take.
            min_value: the maximum value that this attribute can take.
            bins (optional, default 10): the extremities of the bins in which to discretize this attribute.
                If an integer is provided, the domain is divided into `bins` bins of uniform size.
            missing (optional, default False): whether to generate missing values (NaNs) as well.

        The range of this attribute (min and max) is required for the mock generator, which
        discretizes possible values in histogram bins.
        """
        self._add_attribute(
            name,
            AttributeParser.TYPE.CONTINUOUS,
            min_value=min_value,
            max_value=max_value,
            bins=bins,
            allow_missing=missing,
        )

    def add_integer(
        self,
        name: str,
        min_value: int,
        max_value: int,
        bins: Union[int, list] = None,
        missing: bool = False,
    ):
        """
        Adds an integer-valued attribute to the data description.

        Args:
            name: the name of this attribute (column) in the database.
            min_value: the minimum value that this attribute can take.
            min_value: the maximum value that this attribute can take.
            bins (optional, default 10): the extremities of the bins in which to group values.
                If an integer is provided, the domain is divided into `bins` bins of uniform size.
            missing (optional, default False): whether to generate missing values (NaNs) as well.

        Like continuous attributes, the possible values of an integer attribute are grouped
        in consecutive bins. This is to avoid representing distributions with a very large
        number of parameters. This only occurs if bins < max_value - min_value.
        """
        self._add_attribute(
            name,
            AttributeParser.TYPE.INTEGER,
            min_value=min_value,
            max_value=max_value,
            bins=bins,
            allow_missing=missing,
        )

    def add_boolean(self, name: str):
        """
        Adds a boolean-valued attribute to the data description.

        Boolean values are stored as integers with value 0 or 1.

        Args:
            name: the name of this attribute (column) in the database.
        """
        self.add_integer(name, min_value=0, max_value=1, bins=2)

    def add_categorical(self, name: str, possible_values: List[str]):
        """
        Adds a categorical-valued attribute to the data description.

        Args:
            name: the name of this attribute (column) in the database.
            possible_values: the (exhaustive!) list of all possible values that this
                attribute can take.
        """
        self._add_attribute(
            name, AttributeParser.TYPE.CATEGORICAL, possible_values=possible_values
        )

    def add_date(
        self,
        name: str,
        start: str,
        end: str,
        bins: Union[int, List[str]] = None,
        strformat: str = None,
        missing: bool = False,
    ):
        """
        Adds a date column to the data description.

        Args:
            name: the name of this attribute (column) in the database.
            start: the earliest possible date in the dataset.
            end: the latest possible date in the dataset.
            bins (optional, default 10): the extremities of the bins in which to group values.
                If an integer is provided, the domain is divided into `bins` bins of uniform size.
            strformat (optional, default %Y-%m-%d): a formatting str acceptable by datetime.strptime to represent dates.
            missing (optional, default False): whether to generate missing values (empty strings "") as well.
        """
        self._add_attribute(
            name,
            AttributeParser.TYPE.DATE,
            start_date=start,
            end_date=end,
            bins=bins,
            strformat=strformat,
            allow_missing=missing,
        )

    def add_identifier(self, name: str, pattern: str):
        """
        Adds an identifier to the data description.

        Identifiers are columns with unique values, independent from everything else,
        that follow a specific pattern (the only argument of this function).

        The pattern describes IDs with the following characters:
            c: lowercase character,
            C: uppercase character,
            d: digit,
            h: hexadecimal,
        and any other character is kept as is.

        For instance, "ddCC_hh-ccc" could generate "18TI_b3-adw".
        """
        self._add_attribute(name, AttributeParser.TYPE.IDENTIFIER, pattern=pattern)

    def add_name(
        self,
        name,
        pattern: str = None,
        female_proportion: float = None,
        locale: str = None,
    ):
        """
        Adds a name column to the data description.

        Names are identifiers, sampled randomly among the most popular swiss names.

        Args:
            name: the name of this attribute (column) in the database.
            pattern: format of the string, must include {first_name} and/or {last_name}.
                The default is "{first_name} {last_name}".
            female_proportion: the proportion of female names (default 0.5).
            locale: a valid Faker localizer for names (default fr_CH).
        """
        self._add_attribute(
            name,
            AttributeParser.TYPE.NAME,
            pattern=pattern,
            female_proportion=female_proportion,
            locale=locale,
        )


class MeasurementParser:
    """
    Measurement parser for the generic generator.

    `MeasurementParser` objects offer a high-level interface to create and validate the
    part of the configuration of the generic generator that defines measurements.

    A _measurement_ is the result of a query performed on the data (e.g., a histogram of
    values for one attribute). Measurements are typically used in synthetic data to create
    data that replicates important features from the real data. For mock data, these
    measurements are used to constraint the distribution of the mock data.

    """

    class STYLES(str, Enum):
        """Measurement types that are accepted by the generic generator."""

        FULL = "full"
        UNIFORM = "uniform"
        DISTRIBUTION = "distribution"

    def __init__(self, measurement_list: list, attributes: AttributeParser):
        """Internal method -- use `GenericGenerator.measurements`."""
        self.measurement_list = measurement_list
        self.attributes = attributes

    def _add_measurement(self, style, **kwargs):
        # Remove optional arguments (valued None).
        for k, v in list(kwargs.items()):
            if v is None:
                del kwargs[k]
        self.measurement_list.append({"style": style, **kwargs})

    def add_marginal(
        self, variables: List[str], measurement: np.array, noise_scale: float = None
    ):
        """
        Adds a marginal to the measurements.

        Args
            variables (str or list of str): the variables whose k-way marginal is measured.
            measurement (np.array or nested list of floats): the measured marginal for these
                variables. The number of dimensions of this array should be equal to the
                number of variable. Notably, this does not need to be scaled, and can be measured
                with noise (in which case, specify the noise scale).
            noise_scale:

        Raises
            AssertionError: if the number of variables is inconsistent with the dimension
            of the measurement.
        """
        if isinstance(variables, str):
            variables = [variables]
        if isinstance(measurement, np.ndarray):
            measurement = measurement.tolist()
        assert len(variables) == len(np.array(measurement).shape), "Mismatching sizes."
        for name in variables:
            assert name in self.attributes.converters, f"Unknown variable {name}."
        self._add_measurement(
            MeasurementParser.STYLES.FULL,
            variables=variables,
            noisy_marginal=list(measurement),
            noise_scale=noise_scale,
        )

    def add_distribution(
        self, variables, distribution="normal", num_samples=None, **distribution_params
    ):
        """
        Adds a pseudo-measurement defined by a distribution from np.random.

        Args:
            variables (str or list[str]): one or more variable names which follow the distribution.
            distribution (str): the name of a distribution in np.random (i.e., the value [X] such that
                np.random.[X] exists), from which samples of the variables are assumed to be drawn.
            num_samples: number of samples used to estimate the marginal in MBI domain. Higher values
                lead to closer marginals, but are more expensive.
            distribution_params: additional parameters to give to the distribution constructor.
        """
        assert (
            getattr(np.random, distribution, None) is not None
        ), f"Unknown distribution 'np.random.{distribution}'."
        if isinstance(variables, str):
            variables = [variables]
        self._add_measurement(
            MeasurementParser.STYLES.DISTRIBUTION,
            variables=variables,
            distribution=distribution,
            num_samples=num_samples,
            **distribution_params,
        )

    def measure(
        self,
        dataset: pd.DataFrame,
        variables: List[str],
        noise_scale=None,
        laplace=True,
    ):
        """
        Measures a marginal from a pandas DataFrame and use it as measurement.

        Args
            dataset (pd.DataFrame): data from which the marginal is measured.
            variables (str or listr[str]): variables for which to take the k-way marginal.
            noise_scale (float, optional): scale of the noise to add to measured counts, if any.
            laplace (bool, True): if noise_scale is specified, whether to use Laplace or Gaussian
                unbiased noise. By default, Laplace noise is used.
        """
        if isinstance(variables, str):
            variables = [variables]
        if noise_scale is not None:
            assert noise_scale > 0, "Noise scale must be positive."
        # The context class allows to go back-and-forth between data domains.
        context = DatasetContext(
            variables, [self.attributes.converters[v] for v in variables]
        )
        mbi_dataset = context.to_mbi(
            dataset[variables]
        )  # This is a mbi.Dataset object.
        measurement = mbi_dataset.datavector(flatten=False)
        if noise_scale is not None:
            dist = np.random.laplace if laplace else np.random.normal
            measurement += dist(loc=0, scale=noise_scale, size=measurement.shape)
        self.add_marginal(variables, measurement, noise_scale)


class ConstraintParser:
    """
    Constraint parser for the generic generator.

    Constraints modify the configuration of the generator to enforce high-level
    constraints on the data. This can be used to make the data more realistic.

    """

    class TYPES(str, Enum):
        ZEROES = "zeroes"
        CORRELATION = "correlation"

    def __init__(self, attributes: AttributeParser, constraint_list: list):
        """Internal method -- use `GenericGenerator.constraints`."""
        self.attributes = attributes
        self.constraint_list = constraint_list

    def _add_constraint(self, ctype, variables: List[str], **kwargs):
        for v in variables:
            assert v in self.attributes.converters, f"Unknown variable {v}."
        self.constraint_list.append(
            {"type": ctype, "variables": list(variables), **kwargs}
        )

    def apply_structural_zeroes(self, variables: List[str], index: List[int]):
        """
        Requires structural zeroes in specific indices of the dataset.

        This ensures that for all measurements that include at least all these
        variables, the marginal bin corresponding to AND_i variables[i] == index[i]
        is set to zero (in the mbi domain).

        Args
            variables: list of variable names in this dataset.
            index: list of the corresponding value to zero out.

        Examples:
            To enforce that no one can have age in the second bin: variables=["age"], index=[1].
            To enforce that no record can have age in the second bin and sex equal to the first
                value (defined in data format), variables=["age", "sex"], index=[1, 0].
        """
        assert len(variables) == len(
            index
        ), "Mismatch between variables and index length."
        self._add_constraint(
            ConstraintParser.TYPES.ZEROES, variables, index=list(index)
        )

    def apply_correlation(self, variables: List[str], correlation: float):
        """
        Requires that two variables are correlated with some strength.

        Args
            variables: a list of two variables to be made correlated.
            correlation: a parameter in [-1, 1] characterizing the strength and direction of the correlation.

        Disclaimer: the correlation is not strictly speaking the correlation coefficient,
          because other constraints can conflict with this requirement. Furthermore, because of
          the domain discretization, correlation is not well defined in the MBI domain. Treat
          this variable as a proxy for the "strength" of the correlation, with the sign of this
          number indicating the "direction" of the correlation (negative = inversely correlated).
        """
        assert len(variables) == 2, "Correlation can only be between two variables."
        assert (
            -1 <= correlation <= 1
        ), "Correlation coefficient must be between -1 and 1."
        self._add_constraint(
            ConstraintParser.TYPES.CORRELATION, variables, correlation=correlation
        )
