"""Module to generate mock datasets in the Tune Insight instance."""

import json

from enum import Enum

import pandas as pd

from tuneinsight.utils.code import get_code
from tuneinsight.api.sdk.types import Response
from tuneinsight.api.sdk import Client

from tuneinsight.api.sdk.api.api_datagen import post_mock_dataset
from tuneinsight.api.sdk.models.post_mock_dataset_method import PostMockDatasetMethod

from tuneinsight.client.validation import validate_response
from tuneinsight.client.datasource import DataSource


class MockGenerator:
    """Generic class for a Mock generator. Use method-specific subclasses instead."""

    def __init__(self, method: PostMockDatasetMethod):
        self.method = method
        self.datasource = None

    def get_config(self) -> dict:
        """Get the optional configuration for this generator, to be sent in request body as JSON."""
        return {}

    def generate(
        self, client: Client, num_rows: int, table_name: str = None, seed: str = None
    ) -> DataSource:
        """
        Generate a mock dataset.

        Args:
            client (Client): Diapason client instance to connect to the server.
            num_rows (int): number of records to generate.
            table_name (str, optional): name of the database table to generate.
            seed (str, optional): seed of the pseudo-random number generator.

        Raises:
            httpx.TimeoutException: If the request takes longer than Client.timeout.
            InvalidResponseError: If the request to the server fails.

        """
        config: str = json.dumps(self.get_config())
        response: Response = post_mock_dataset.sync_detailed(
            client=client,
            json_body=config,
            method=self.method,
            name=table_name or f"mock_{self.method}",
            numrows=num_rows,
            seed=seed,
        )
        validate_response(response=response)
        # The response contains the description of the datasource created by the call.
        self.datasource = DataSource(model=response.parsed, client=client)
        return self.datasource

    @property
    def df(self) -> pd.DataFrame:
        """Download the last dataframe generated by this generator."""
        if self.datasource is None:
            raise RuntimeError("No Dataframe found. Run the generator first.")
        return self.datasource.get_dataframe(
            query=f"SELECT * FROM {self.datasource.model.name}"
        )


class AlertsGenerator(MockGenerator):
    """Mock Alerts generation.

    This produces mock alerts from a network monitoring system. This has the attributes:
     - src_ip: source IP address (str).
     - dst_ip: destination IP address (str).
     - protocol: one of TCP, UDP or ICMP (str).
     - fingerprint: a SHA hash (str).
     - type: the attack type (str). Also 1-hot encoded.
     - severity: one of low, medium, high or critical (str). Also 1-hot encoded.

    """

    def __init__(self):
        MockGenerator.__init__(self, PostMockDatasetMethod.ALERTS)


class PatientsGenerator(MockGenerator):
    """Mock Patients generation.

    This produces mock simple patients records that can be used for survival analysis.
    The data has the following attributes:
     - patient_no: a pseudorandom identifier (str).
     - age: patient age (int).
     - sex: one of "female" or "male" (str).
     - height: patient height in cm (float).
     - weight: patient weight in kg (float).
     - observation: a disease/symptom, or empty string (str).
     - treatments: treatment received by the patient, or empty string (str).
     - diagnosis: timestamp at which an observation was recorded (timestamp).
     - death: timestamp at which the patient died or NaT if not dead (timestamp).

    """

    def __init__(self):
        MockGenerator.__init__(self, PostMockDatasetMethod.PATIENTS)


class NeurologyObservationsGenerator(MockGenerator):
    """Mock Neurology Observations Generation

    This produces mock neurological observations for patients. It has the attributes:
     - patient_id: a pseudorandom identifier (str).
     - disease_type: a neurological disease (str, 5 possible values).
     - diagnosis_dt: time of diagnosis (timestamp).
     - birthdate: birth date of the patient (timestamp).
     - pain_score: float between 0 and 11, or empty string (str).
     - mri_anomaly_detected: one of "NaN", "yes" or "no" (str).
     - surgery_required: one of "yes", "no", "" (str).

    """

    def __init__(self):
        MockGenerator.__init__(self, PostMockDatasetMethod.NEUROLOGY_OBSERVATIONS)


class PricesGenerator(MockGenerator):
    """Mock Prices Generation

    This produces a dataset of mock prices for commodities at a given time.
     - commodity (str): what is being sold.
     - delivery_start, delivery_end (date): period of delivery for this commodity.
     - currency (str): currency in which the price is expressed.
     - currency_multiplier (float): conversion rate to USD for the currency.
     - unit (str): unit in which the quantity is expressed.
     - unit_multiplier (float): conversion from the unit to a standard unit.
     - contributor (str): who is selling.
     - price (float): price per unit at which the commodity is sold.

    """

    def __init__(self):
        MockGenerator.__init__(self, PostMockDatasetMethod.PRICES)


class SKUGenerator(MockGenerator):
    """Mock SKU Generation

    This produces mock stock keeping units (grocery products) with the following attributes:
     - sku_number: pseudorandom numerical value that is generated from the name of the product
     - product_name: name of the product.
     - product_type: type of product (spaghetti,chocolate etc...)
     - category: higher level category for the product (meat,drinks,dairy etc...)
     - manufacturer: name of the manufacturer.
     - supplier: name of the supplier.
     - price: price of individual product.
     - quantity: quantity produced for the current timestamp.
     - timestamp: the current timestamp.

    """

    def __init__(self):
        MockGenerator.__init__(self, PostMockDatasetMethod.SKUS)


class PersonsGenerator(MockGenerator):
    """Mock Persons Generation

    This produces a mock dataset representing individuals with the following attributes:
     - name: full name of the person.
     - age: age of the person.
     - country: country in which the person lives.

    """

    def __init__(self):
        MockGenerator.__init__(self, PostMockDatasetMethod.PERSONS)


class CustomFunctionGenerator(MockGenerator):
    """Custom Function Generation

    This generator enables users to specify an arbitrary Python function to generate
    mock records. The signature of the function must be () -> pd.DataFrame. The
    DataFrame it outputs contains one or more records related to one "user". This
    function is called until enough records are produced (if too many records are
    produced, the last output is downsampled such that the mock dataset has exactly
    the required size).

    The custom function should not use `import` statements, but can use the following
    libraries without importing them: datetime, dateutil, math, random, itertools,
    np (numpy), pd (pandas).

    The code of the custom function should be randomized and not seeded, as the
    generator takes care of seeding.

    Note: you can and should use this class as a decorator over the custom function.

    """

    def __init__(self, f):
        MockGenerator.__init__(self, PostMockDatasetMethod.CUSTOM_FUNCTION)
        self.f = f

    def get_config(self):
        return {"function": get_code(self.f)}

    def __call__(self):
        """Call the inner function of this generator (for decoration purposes)."""
        return self.f()


class GenericGenerator(MockGenerator):
    """Mock generator for arbitrary data

    This generator can be configured to produce mock tabular data of any format, and
    to add arbitrary distributions and correlations for attributes.

    This generator requires a data format describing the attributes. This can be either
    from a JSON file, or by manually adding columns through method columns.

    Additionally, "measurements" (the simulated results of queries) can be provided
    to describe the distribution of the mock data.

    """

    def __init__(self, data_format=None):
        MockGenerator.__init__(self, PostMockDatasetMethod.GENERIC)
        self.data_format = data_format if data_format is not None else []
        # Validation of the data format and addition of attributes is done through the
        # self.attributes object of the generator.
        self.attrs = self.attributes = _AttributeParser(self.data_format)

    @classmethod
    def from_file(cls, data_format_file):
        """Load a Generic generator from a data format described in a JSON file."""
        with open(data_format_file, "r", encoding="utf-8") as ff:
            data_format = json.load(ff)
        return GenericGenerator(data_format)

    def save(self, json_filename):
        """Save the data format of this generator to a JSON file."""
        with open(json_filename, "w", encoding="utf-8") as ff:
            json.dump(self.data_format, ff)

    def get_config(self):
        return {"data-format": self.data_format}


class _AttributeParser:
    """Attribute parser for the generic mock generator.

    _AttributeParser objects ensure that the arguments sent to the generic generator are
    valid, and allows users to add new attributes through a safe and easy interface.

    """

    class TYPE(str, Enum):
        """Attribute types that are recognized by the generic generator."""

        CONTINUOUS = "continuous"
        INTEGER = "integer"
        CATEGORICAL = "categorical"

    # Maps the name of the attribute to a pair of (required, optional) parameters.
    FORMAT = {
        TYPE.CONTINUOUS: (
            {"min_value", "max_value"},
            {
                "num_bins",
            },
        ),
        TYPE.INTEGER: (
            {"min_value", "max_value"},
            {
                "num_bins",
            },
        ),
        TYPE.CATEGORICAL: ({"possible_values"}, set()),
    }

    def __init__(self, data_format):
        # Check that the data format is valid.
        self.validate(data_format)
        # The *object* itself (a list).
        self.data_format = data_format

    def validate(self, list_of_attributes: list[dict]):
        """Assert that a data format is valid. Raises error if not."""
        for attr in list_of_attributes:
            assert "name" in attr, "Missing attribute name."
            assert "type" in attr, "Missing attribute type."
            c = _AttributeParser.FORMAT.get(attr["type"])
            assert c is not None, f"Unknown type {c}."
            required, optional = c
            for req in required:
                assert req in attr, f"Missing {req} attribute for type {attr['type']}."
            allowed = required.union(optional.union({"name", "type"}))
            for v in attr:
                assert (
                    v in allowed
                ), f"Attribute {v} not allowed for type {attr['type']}."

    def _add_attribute(self, name: str, atype: TYPE, **attributes):
        attr_values = {"name": name, "type": atype, **attributes}
        self.validate([attr_values])
        # Remove optional arguments from the payload.
        # This is because providing a None (null) value will overwrite defaults.
        for optional_arg in _AttributeParser.FORMAT[atype][1]:
            if optional_arg in attributes and attributes[optional_arg] is None:
                del attr_values[optional_arg]
        self.data_format.append(attr_values)

    # The following methods are intended as a public interface to this class.

    def add_continuous(
        self, name: str, min_value: float, max_value: float, num_bins: int = None
    ):
        """
        Add a continuous-valued attribute to the data description.

        Args:
            name: the name of this attribute (column) in the database.
            min_value: the minimum value that this attribute can take.
            min_value: the maximum value that this attribute can take.
            num_bins (optional, default 10): number of bins to discretize this attribute into.

        The range of this attribute (min and max) is required for the mock generator, which
        discretizes possible values in histogram bins.

        """
        self._add_attribute(
            name,
            _AttributeParser.TYPE.CONTINUOUS,
            min_value=min_value,
            max_value=max_value,
            num_bins=num_bins,
        )

    def add_integer(
        self, name: str, min_value: int, max_value: int, num_bins: int = None
    ):
        """
        Add an integer-valued attribute to the data description.

        Args:
            name: the name of this attribute (column) in the database.
            min_value: the minimum value that this attribute can take.
            min_value: the maximum value that this attribute can take.
            num_bins (optional, default 10): number of bins to group possible values.

        Like continuous attributes, the possible values of an integer attribute are grouped
        in consecutive bins. This is to avoid representing distributions with a very large
        number of parameters. This only occurs if num_bins < max_value - min_value.

        """
        self._add_attribute(
            name,
            _AttributeParser.TYPE.INTEGER,
            min_value=min_value,
            max_value=max_value,
            num_bins=num_bins,
        )

    def add_categorical(self, name: str, possible_values: list[str]):
        """
        Add a categorical-valued attribute to the data description.

        Args:
            name: the name of this attribute (column) in the database.
            possible_values: the (exhaustive!) list of all possible values that this
                attribute can take.

        """
        self._add_attribute(
            name, _AttributeParser.TYPE.CATEGORICAL, possible_values=possible_values
        )
